import os
import random
import time
from typing import List, Any, Tuple, Callable
from tqdm import tqdm

from config.config import get_random_headers, setup_driver
from config.logger import logger
from processed.cache import load_vacancy_from_cache, save_vacancy_to_cache
from models.models import JobDetail



class VacancyProcessor:
    def __init__(self):
        self.driver = setup_driver()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.driver.quit()

    def collect_metadata(self, scraper_cls, url: str, site_name: str) -> Tuple[List[str], int, int]:

        logger.info(f"\n{'=' * 50}")
        logger.info(f"üåê –°—Ç–∞—Ä—Ç –æ–±—Ä–æ–±–∫–∏ {site_name}")

        scraper = scraper_cls(url, self.driver)

        logger.info("üìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–æ–∫...")
        html_pages = scraper.get_all_pages_html()

        num_pages = len(html_pages)
        logger.info(f"üî¢ –û—Ç—Ä–∏–º–∞–Ω–æ {num_pages} —Å—Ç–æ—Ä—ñ–Ω–æ–∫")

        logger.info("üîó –ó–±—ñ—Ä –ø–æ—Å–∏–ª–∞–Ω—å...")
        links = scraper.get_all_links(html_pages)

        num_links = len(links)
        logger.info(f"üîó –ó–Ω–∞–π–¥–µ–Ω–æ {num_links} –ø–æ—Å–∏–ª–∞–Ω—å")

        return links, num_pages, num_links

    def process_vacancies(
            self,
            links: list[str],
            parser_vacancy_func: Callable[[str, str], JobDetail],
            site_name: str) -> list[JobDetail]:

        logger.info(f"–ü–æ—á–∞—Ç–æ–∫ –æ–±—Ä–æ–±–∫–∏ {len(links)} –≤–∞–∫–∞–Ω—Å—ñ–π –∑ {site_name}")
        vacancies = []
        success_count = 0
        with tqdm(total=len(links), desc=f"–û–±—Ä–æ–±–∫–∞ {site_name}", unit="vac") as pbar:
            for i, link in enumerate(links, 1):
                try:
                    logger.info("–û–±—Ä–æ–±–∫–∞ %d/%d: %s", i, len(links), link)
                    cached_data = load_vacancy_from_cache(link)
                    if cached_data is not None:
                        vacancy = JobDetail(**cached_data)
                        logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ –∫–µ—à—É: {link}")
                    else:
                        headers = get_random_headers()
                        self.driver.execute_cdp_cmd("Network.setExtraHTTPHeaders", {"headers": headers})
                        logger.info(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è User-Agent: {headers['User-Agent']}")

                        self.driver.get(link)
                        time.sleep(random.uniform(0.5, 0.8))
                        html = self.driver.page_source
                        vacancy = parser_vacancy_func(link, html)
                        if vacancy:
                            save_vacancy_to_cache(link, vacancy)
                            logger.info(f"\n–ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –∫–µ—à: {link}")
                    if vacancy:
                        vacancies.append(vacancy)
                        success_count += 1
                    else:
                        logger.warning(f"–ü–æ—Ä–æ–∂–Ω—ñ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è {link}")

                    pbar.set_postfix_str(f"–û—Å—Ç–∞–Ω–Ω—è: {link[:30]}...")
                    pbar.update(1)
                except Exception as e:
                    logger.error(f"–ü–æ–º–∏–ª–∫–∞ —É {link}: {str(e)}")
                    pbar.update(1)
                    continue
        logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ {site_name}: {len(vacancies)}/{len(links)} —É—Å–ø—ñ—à–Ω–∏—Ö")
        return vacancies


def collect_vacancies_from_site(
        scraper_cls,
        url: str,
        parser_vacancy_func,
        site_name: str,

) -> List[Any]:
    with VacancyProcessor() as processor:
        try:
            links, _, _ = processor.collect_metadata(scraper_cls, url, site_name)
            return processor.process_vacancies(
                links=links,
                parser_vacancy_func=parser_vacancy_func,
                site_name=site_name
            )
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–æ—Ä—ñ –≤–∞–∫–∞–Ω—Å—ñ–π –∑ {site_name}: {e}", exc_info=True)
            return []
